{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "### config.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import config.config_ as c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../data/train.xlsx'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PACKAGE_ROOT = r'/home/eashan/Eashan/Data_Science/NLP/nlp/project_multilingual/french_classifier/custom_pipeline/pipeline/ipynb_files_3/python_files'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINED_MODEL_DIR = PACKAGE_ROOT +  '/trained_models'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/eashan/Eashan/Data_Science/NLP/nlp/project_multilingual/french_classifier/custom_pipeline/pipeline/ipynb_files_3/python_files/trained_models'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TRAINED_MODEL_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_DIR = PACKAGE_ROOT + '/data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/eashan/Eashan/Data_Science/NLP/nlp/project_multilingual/french_classifier/custom_pipeline/pipeline/ipynb_files_3/python_files/data'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATASET_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data\n",
    "TESTING_DATA_FILE = 'test.xlsx'\n",
    "TRAINING_DATA_FILE = 'train.xlsx'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipeline name\n",
    "PIPELINE_NAME = 'Random_Forest_pipeline'\n",
    "\n",
    "\n",
    "# features\n",
    "FEATURES = 'Résumé'\n",
    "# FEATURES = 'Descriptif' # The reason being french data not captured in json\n",
    "\n",
    "# target\n",
    "TARGET = 'SubCat Translated'\n",
    "\n",
    "path = \"./data/train.xlsx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_management.py\n",
    "# waiting to check it's working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eashan/anaconda3/envs/nlp/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from config import config_ as config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(*, file_name: str\n",
    "                 ) -> pd.DataFrame:\n",
    "    _data = pd.read_excel(f'{DATASET_DIR}/{file_name}')\n",
    "    return _data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tr = load_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_pipeline(*, piplen(subject.get('predictions'))eline_to_persist) -> None:\n",
    "    \"\"\"Persist the pipeline.\"\"\"\n",
    "\n",
    "#     save_file_name = 'regression_model.pkl'\n",
    "    save_path = TRAINED_MODEL_DIR + '/' + PIPELINE_NAME\n",
    "    joblib.dump(pipeline_to_persist, save_path)\n",
    "\n",
    "    print('saved pipeline')\n",
    "len(subject.get('predictions'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_file_name = 'regression_model.pkl'\n",
    "# save_path = TRAINED_MODEL_DIR + '/' + save_file_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/eashan/Eashan/Data_Science/NLP/nlp/project_multilingual/french_classifier/custom_pipeline/pipeline/ipynb_files_3/python_files/trained_models/regression_model.pkl'"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pipeline(*, file_name: str\n",
    "                  ) -> Pipeline:\n",
    "    \"\"\"Load a persisted pipeline.\"\"\"\n",
    "\n",
    "    file_path = TRAINED_MODEL_DIR + '/' + file_name\n",
    "    saved_pipeline = joblib.load(filename=file_path)\n",
    "    return saved_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import class_weight\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classweights_sklearn(path):\n",
    "    dat = pd.read_excel(path)   \n",
    "    dat[TARGET].fillna('other', inplace=True)\n",
    "    label = dat[TARGET].astype(str).values.tolist()\n",
    "\n",
    "    class_weights = class_weight.compute_class_weight('balanced', np.unique(label), label)\n",
    "\n",
    "    cw = dict(zip(np.unique(label), class_weight.compute_class_weight('balanced',\n",
    "                                                 np.unique(label),\n",
    "                                                 label))) \n",
    "    return cw\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessers.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from nltk.stem.snowball import FrenchStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Regex(BaseEstimator, TransformerMixin):\n",
    "#  https://stackoverflow.com/questions/52123026/sklearn-pipeline-valueerror-could-not-convert-string-to-float   \n",
    "    def fit(self, X, y = None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, text):\n",
    "        for i in range(0, len(text)):\n",
    "            text[i] = text[i].lower()\n",
    "            text[i] = re.sub('\\d{2}/\\d{2}/\\d{4}', ' ', text[i])\n",
    "            text[i] = re.sub(r'\\d{2}:\\d{2}:\\d{2}', ' ', text[i])\n",
    "            text[i] = re.sub(r\"[a-z0-9]+[\\.'\\-]*[a-z0-9]+@(laposte|googlemail)\\.(com|fr)$\", ' ', text[i])\n",
    "            text[i] = re.sub(r'[?|$|.|!|-|#|*|&|/|%|,|\"|:|;|*|@|#|\\[|\\]|\\(|\\)]',r'',text[i])\n",
    "        return text\n",
    "\n",
    "\n",
    "# In[14]:\n",
    "\n",
    "\n",
    "class Fr_Stemmer(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def fit(self, X, y = None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, corpus):\n",
    "        st = FrenchStemmer()\n",
    "        stem = []\n",
    "        for sentence in corpus:\n",
    "            stem.append(\" \".join([st.stem(i) for i in sentence.split()]))\n",
    "        return stem\n",
    "\n",
    "\n",
    "# In[15]:\n",
    "\n",
    "\n",
    "class df_tolist(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self, feature = None):\n",
    "        self.feature = feature\n",
    "    \n",
    "    def fit(self, X, y = None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, corpus):\n",
    "        corpus = corpus.copy()\n",
    "        dat = corpus[self.feature].values.tolist()\n",
    "        return dat\n",
    "\n",
    "\n",
    "# In[16]:\n",
    "\n",
    "\n",
    "class fill_na(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self, feature = None):\n",
    "        self.feature = feature\n",
    "    \n",
    "    def fit(self, X, y = None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, corpus):\n",
    "        corpus = corpus.copy()\n",
    "        corpus[self.feature].fillna('other', inplace=True)\n",
    "        return corpus\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from imblearn.pipeline import Pipeline  ### use this one only\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from imblearn.over_sampling import SMOTE\n",
    "# import preprocessers as pp\n",
    "# import class_weight as cw\n",
    "# import config_ as cf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[3]:\n",
    "# cw needs to be inserted\n",
    "pipe = Pipeline([\n",
    " ('na', fill_na(FEATURES)),\n",
    " ('lis', df_tolist(FEATURES)),\n",
    " ('reg', Regex()),\n",
    " ('stem', Fr_Stemmer()),\n",
    " ('vect', CountVectorizer(ngram_range = (1, 1))),\n",
    " ('tfidf', TfidfTransformer(use_idf = False)),\n",
    " ('sm', SMOTE(sampling_strategy = 'minority', random_state=32, n_jobs = -1, k_neighbors = 1)), \n",
    " ('rfc', RandomForestClassifier(n_estimators = 1200, criterion = 'entropy',\n",
    "                                random_state = 32, max_depth = 80, min_samples_split = 2, \n",
    "                                min_samples_leaf = 1, class_weight = classweights_sklearn(f'{DATASET_DIR}/{TRAINING_DATA_FILE}')) )])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_pipeline.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# from regression_model import pipeline\n",
    "# from regression_model.processing.data_management import (\n",
    "#     load_dataset, save_pipeline)\n",
    "# from regression_model.config import config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_training() -> None:\n",
    "    \"\"\"Train the model.\"\"\"\n",
    "\n",
    "    # read training data\n",
    "    data = load_dataset(file_name= TRAINING_DATA_FILE)\n",
    "    \n",
    "    # divide train and test\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data[[FEATURES]],\n",
    "    data[[TARGET]].values,\n",
    "    test_size=0.1,\n",
    "    random_state=32)  # we are setting the seed here\n",
    "    \n",
    "    # transform the target\n",
    "#     y_train = np.log(y_train)\n",
    "#     y_test = np.log(y_test)\n",
    "\n",
    "    pipe.fit(X_train, y_train)\n",
    "\n",
    "    save_pipeline(pipeline_to_persist=pipe)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved pipeline\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    run_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validation.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from regression_model.config import config\n",
    "\n",
    "# import pandas as pd\n",
    "\n",
    "\n",
    "def validate_inputs(input_data: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Check model inputs for unprocessable values.\"\"\"\n",
    "\n",
    "    validated_data = input_data.copy()\n",
    "    val_data = validated_data[[FEATURES]]\n",
    "\n",
    "    # check for numerical variables with NA not seen during training\n",
    "#     if input_data[config.NUMERICAL_NA_NOT_ALLOWED].isnull().any().any():\n",
    "#         validated_data = validated_data.dropna(\n",
    "#             axis=0, subset=config.NUMERICAL_NA_NOT_ALLOWED)\n",
    "\n",
    "    # check for categorical variables with NA not seen during training\n",
    "#     if input_data[config.CATEGORICAL_NA_NOT_ALLOWED].isnull().any().any():\n",
    "#         validated_data = validated_data.dropna(\n",
    "#             axis=0, subset=config.CATEGORICAL_NA_NOT_ALLOWED)\n",
    "\n",
    "    # check for values <= 0 for the log transformed variables\n",
    "#     if (input_data[config.NUMERICALS_LOG_VARS] <= 0).any().any():\n",
    "#         vars_with_neg_values = config.NUMERICALS_LOG_VARS[\n",
    "#             (input_data[config.NUMERICALS_LOG_VARS] <= 0).any()]\n",
    "#         validated_data = validated_data[\n",
    "#             validated_data[vars_with_neg_values] > 0]\n",
    "\n",
    "    return validated_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from regression_model.processing.data_management import load_pipeline\n",
    "# from regression_model.config import config\n",
    "# from regression_model.processing.validation import validate_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_file_name = PIPELINE_NAME\n",
    "_category_pipe = load_pipeline(file_name=pipeline_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_prediction(*, input_data) -> dict:\n",
    "    \"\"\"Make a prediction using the saved model pipeline.\"\"\"\n",
    "\n",
    "    data = pd.read_json(input_data)\n",
    "    validated_data = validate_inputs(input_data=data)\n",
    "    prediction = _category_pipe.predict(validated_data[[FEATURES]])\n",
    "#     output = np.exp(prediction)\n",
    "    response = {'predictions': prediction}\n",
    "\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "# from regression_model.predict import make_prediction\n",
    "# from regression_model.processing.data_management import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_make_single_prediction():\n",
    "    # Given\n",
    "    test_data = load_dataset(file_name=TESTING_DATA_FILE)\n",
    "    single_test_json = test_data[0:1].to_json(orient='records', force_ascii = False)\n",
    "\n",
    "    # When\n",
    "    subject = make_prediction(input_data=single_test_json)\n",
    "\n",
    "    # Then\n",
    "    assert subject is not None\n",
    "    assert isinstance(subject.get('predictions')[0], str)\n",
    "#     assert math.ceil(subject.get('predictions')[0]) == 112476\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_make_multiple_predictions():\n",
    "    # Given\n",
    "    test_data = load_dataset(file_name=TESTING_DATA_FILE)\n",
    "    original_data_length = len(test_data)\n",
    "    multiple_test_json = test_data.to_json(orient='records', force_ascii = False)\n",
    "\n",
    "    # When\n",
    "    subject = make_prediction(input_data=multiple_test_json)\n",
    "\n",
    "    # Then\n",
    "    assert subject is not None\n",
    "    assert len(subject.get('predictions')) == 7646\n",
    "\n",
    "    # We expect some rows to be filtered out\n",
    "    assert len(subject.get('predictions')) != original_data_length\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Résumé</th>\n",
       "      <th>Descriptif</th>\n",
       "      <th>SubCat Translated</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I08249435 - 4 - AP0345 - DISK WARNING sur CPYV...</td>\n",
       "      <td>[CANOPSIS]\\nMessage d'erreur :\\n05/02/2019\\n17...</td>\n",
       "      <td>Autre</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I08249442 - 4 - AP0345 - DISK WARNING - free s...</td>\n",
       "      <td>[CANOPSIS]\\nMessage d'erreur :\\n05/02/2019\\n17...</td>\n",
       "      <td>Autre</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I08250866 - 4 - AP0345 - APP_PCTESPLIBRE_ADMAA...</td>\n",
       "      <td>[CANOPSIS]\\nMessage d'erreur :\\nnull\\nnull\\n15...</td>\n",
       "      <td>Autre</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Résumé  \\\n",
       "0  I08249435 - 4 - AP0345 - DISK WARNING sur CPYV...   \n",
       "1  I08249442 - 4 - AP0345 - DISK WARNING - free s...   \n",
       "2  I08250866 - 4 - AP0345 - APP_PCTESPLIBRE_ADMAA...   \n",
       "\n",
       "                                          Descriptif SubCat Translated  \n",
       "0  [CANOPSIS]\\nMessage d'erreur :\\n05/02/2019\\n17...             Autre  \n",
       "1  [CANOPSIS]\\nMessage d'erreur :\\n05/02/2019\\n17...             Autre  \n",
       "2  [CANOPSIS]\\nMessage d'erreur :\\nnull\\nnull\\n15...             Autre  "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = load_dataset(file_name= TESTING_DATA_FILE)\n",
    "test_data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_test_json = test_data[0:1].to_json(orient='records', force_ascii = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[{\"Résumé\":\"I08249435 - 4 - AP0345 - DISK WARNING sur CPYV0621\",\"Descriptif\":\"[CANOPSIS]\\\\nMessage d\\'erreur :\\\\n05\\\\/02\\\\/2019\\\\n17:14:47 05\\\\/02\\\\/2019\\\\n17:14:47 icinga2 fk_ CPYV0621 APP_PCTESPLIBRE_ADMAA\\\\nDISK WARNING - free space: \\\\/fk_\\\\/fk_admaa 14%\\\\nnull\\\\nProcédure ou FA suivie :LZ_LINUX_PCTESPLIBRE_CRI_SCI\\\\nhttps:\\\\/\\\\/wiki.net-courrier.extra.laposte.fr\\\\/confluence\\\\/display\\\\/EXPLOITATION\\\\/LZ_LINUX_PCTESPLIBRE_CRI_SCI\\\\nnull\\\\nRien dans :\\\\nConsigne FS\\\\nhttps:\\\\/\\\\/wiki.net-courrier.extra.laposte.fr\\\\/confluence\\\\/display\\\\/EXPLOITATION\\\\/Consigne+FS\\\\n\",\"SubCat Translated\":\"Autre\"}]'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "single_test_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_json(single_test_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Descriptif</th>\n",
       "      <th>Résumé</th>\n",
       "      <th>SubCat Translated</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[CANOPSIS]\\nMessage d'erreur :\\n05/02/2019\\n17...</td>\n",
       "      <td>I08249435 - 4 - AP0345 - DISK WARNING sur CPYV...</td>\n",
       "      <td>Autre</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          Descriptif  \\\n",
       "0  [CANOPSIS]\\nMessage d'erreur :\\n05/02/2019\\n17...   \n",
       "\n",
       "                                              Résumé SubCat Translated  \n",
       "0  I08249435 - 4 - AP0345 - DISK WARNING sur CPYV...             Autre  "
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject = make_prediction(input_data=single_test_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'predictions': array(['Autre'], dtype=object)}"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "isinstance(subject.get('predictions')[0], str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(subject.get('predictions'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
